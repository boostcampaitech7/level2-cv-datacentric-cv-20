{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\")\n",
    "\n",
    "train_img_data = dataset['train']['image']\n",
    "val_img_data = dataset['validation']['image']\n",
    "test_img_data = dataset['test']['image']\n",
    "\n",
    "train_json_data = dataset['train']['ground_truth']\n",
    "val_json_data = dataset['validation']['ground_truth']\n",
    "test_json_data = dataset['test']['ground_truth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORD 이미지 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../CORD_dataset/train', exist_ok=True)\n",
    "os.makedirs('../CORD_dataset/val', exist_ok=True)\n",
    "\n",
    "for i in range(len(train_img_data)):\n",
    "    file_name = f'../CORD_dataset/train/CORD_dataset_train_{str(i).zfill(4)}.png'\n",
    "    train_img_data[i].save(file_name)\n",
    "\n",
    "for i in range(len(val_img_data)):\n",
    "    file_name = f'../CORD_dataset/val/CORD_dataset_val_{str(i).zfill(4)}.png'\n",
    "    val_img_data[i].save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORD JSON -> UFO format으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cord_json_to_ufo(type, output_path):\n",
    "    ufo_format = {'images' : {}}\n",
    "    if type == 'train':\n",
    "        json_data = train_json_data\n",
    "    elif type == 'val':\n",
    "        json_data = val_json_data\n",
    "    else:\n",
    "        json_data = test_json_data\n",
    "        \n",
    "    for id, img_data in enumerate(json_data):\n",
    "        img_name = f'CORD_dataset_{type}_{str(id).zfill(4)}.png'\n",
    "\n",
    "        if img_name not in ufo_format['images']:\n",
    "            ufo_format['images'][img_name] = {'words': {}}\n",
    "        \n",
    "        word_id = 0\n",
    "\n",
    "        for word_data in eval(img_data)['valid_line']:\n",
    "            for word_info in word_data['words']:\n",
    "\n",
    "                quad = word_info['quad']\n",
    "                points = [\n",
    "                    [quad['x1'], quad['y1']],\n",
    "                    [quad['x2'], quad['y2']],\n",
    "                    [quad['x3'], quad['y3']],\n",
    "                    [quad['x4'], quad['y4']]\n",
    "                ]\n",
    "\n",
    "                word_index = f'{str(word_id+1).zfill(4)}'\n",
    "\n",
    "                ufo_annotation = {\n",
    "                    'transcription': word_info['text'],\n",
    "                    'points': points\n",
    "                }\n",
    "\n",
    "                ufo_format['images'][img_name]['words'][word_index] = ufo_annotation\n",
    "\n",
    "                word_id += 1\n",
    "\n",
    "        ufo_format['images'][img_name]['img_w'] = eval(img_data)['meta']['image_size']['width']\n",
    "        ufo_format['images'][img_name]['img_h'] = eval(img_data)['meta']['image_size']['height']\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(ufo_format, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_json_to_ufo('train', '../CORD_dataset/train.json')\n",
    "cord_json_to_ufo('val', '../CORD_dataset/val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORD_dataset, original dataset 합치기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋이 성공적으로 합쳐졌습니다.\n"
     ]
    }
   ],
   "source": [
    "cord_dataset_path = \"../CORD_dataset\"\n",
    "dataset_path = \"../dataset\"\n",
    "\n",
    "# 새롭게 합친 데이터셋 저장 경로\n",
    "new_dataset_path = \"../Ori_CORD_dataset\"\n",
    "os.makedirs(new_dataset_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_path, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_path, \"val\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_path, \"test\"), exist_ok=True)\n",
    "\n",
    "def copy_images(src_folder, dest_folder):\n",
    "    for filename in os.listdir(src_folder):\n",
    "        src_path = os.path.join(src_folder, filename)\n",
    "        dest_path = os.path.join(dest_folder, filename)\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "\n",
    "copy_images(os.path.join(cord_dataset_path, \"train\"), os.path.join(new_dataset_path, \"train\"))\n",
    "copy_images(os.path.join(dataset_path, \"train\"), os.path.join(new_dataset_path, \"train\"))\n",
    "\n",
    "copy_images(os.path.join(cord_dataset_path, \"val\"), os.path.join(new_dataset_path, \"val\"))\n",
    "\n",
    "copy_images(os.path.join(dataset_path, \"test\"), os.path.join(new_dataset_path, \"test\"))\n",
    "\n",
    "def merge_json_files(json_files):\n",
    "    merged_data = {\"images\": {}}\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            merged_data[\"images\"].update(data[\"images\"])\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "train_json_files = [\n",
    "    os.path.join(cord_dataset_path, \"train.json\"),\n",
    "    os.path.join(dataset_path, \"train.json\")\n",
    "]\n",
    "merged_train_json = merge_json_files(train_json_files)\n",
    "\n",
    "val_json_src = os.path.join(cord_dataset_path, \"val.json\")\n",
    "test_json_src = os.path.join(dataset_path, \"test.json\")\n",
    "\n",
    "with open(os.path.join(new_dataset_path, \"train.json\"), \"w\") as f:\n",
    "    json.dump(merged_train_json, f)\n",
    "\n",
    "shutil.copy2(val_json_src, os.path.join(new_dataset_path, \"val.json\"))\n",
    "shutil.copy2(test_json_src, os.path.join(new_dataset_path, \"test.json\"))\n",
    "\n",
    "\n",
    "if len(os.listdir(os.path.join(new_dataset_path, 'train'))) == 1200 and\\\n",
    "    len(os.listdir(os.path.join(new_dataset_path, 'val'))) == 100 and\\\n",
    "    len(os.listdir(os.path.join(new_dataset_path, 'test'))) == 120:\n",
    "    print('Train, Val, and Test successfully created!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
